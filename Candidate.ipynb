{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-phone",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-07T06:08:58.090957Z",
     "iopub.status.busy": "2021-04-07T06:08:58.089396Z",
     "iopub.status.idle": "2021-04-07T06:08:59.844473Z",
     "shell.execute_reply": "2021-04-07T06:08:59.842883Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from circuit import Circuit\n",
    "import numpy as np\n",
    "import dgl.function as fn\n",
    "import random\n",
    "import networkx as nx\n",
    "from utils import *\n",
    "from dgl.data.utils import save_graphs, load_graphs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "small-dairy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-07T06:08:59.909271Z",
     "iopub.status.busy": "2021-04-07T06:08:59.903813Z",
     "iopub.status.idle": "2021-04-07T06:09:17.717032Z",
     "shell.execute_reply": "2021-04-07T06:09:17.715481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start parsing verilog netlist\n",
      "nodeID: 1086147\n",
      "End parsing verilog netlist\n",
      "CPU time: 12.77s\n",
      "\n",
      "Start parsing verilog netlist\n",
      "nodeID: 1948752\n",
      "End parsing verilog netlist\n",
      "CPU time: 11.95s\n",
      "\n",
      "Start parsing top verilog netlist\n",
      "End parsing verilog netlist\n",
      "CPU time: 8.14s\n",
      "\n",
      "Start parsing STIL patterns\n",
      "Pass Pattern 0\n",
      "Final Pat\n",
      "End parsing STIL patterns\n",
      "CPU time: 16.94s\n",
      "\n",
      "Graph(num_nodes={'faultSite': 2715084, 'topNode': 108720},\n",
      "      num_edges={('faultSite', 'net', 'faultSite'): 2844138, ('topNode', 'topEdge', 'faultSite'): 617775554},\n",
      "      metagraph=[('faultSite', 'faultSite', 'net'), ('topNode', 'faultSite', 'topEdge')])\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "preprocess_begin = time.time()\n",
    "\n",
    "design = \"leon3_GNN\"\n",
    "path = design+\"/heterograph_600.bin\"\n",
    "\n",
    "cir = Circuit(design)\n",
    "root = \"/autofs/home/sh528/M3Ddesigns/\"+design+\"/\"\n",
    "cir.parseHierVerilog(root+\"die0.v\")\n",
    "cir.parseHierVerilog(root+\"die1.v\")\n",
    "cir.parseTop(root+\"top.v\")\n",
    "# cir.parseVerilog(design+\"/\"+design+\".v\")\n",
    "# cir.parsePartition(design+\"/die0.rpt\")\n",
    "stil = design+\"/TDF_600.stil\"\n",
    "\n",
    "# if False:\n",
    "if os.path.isfile(path):\n",
    "    n_patterns = cir.parseSTIL(stil, -2)\n",
    "    dic, topEdge = backprop(cir,False)\n",
    "    hg = load_graphs(path)[0][0]\n",
    "else:\n",
    "    n_patterns = cir.parseSTIL(stil)\n",
    "    edge = CreateGraphByFaultSite(cir)\n",
    "    dic, topEdge = backprop(cir)\n",
    "    \n",
    "\n",
    "    hg = dgl.heterograph({ ('topNode', 'topEdge', 'faultSite'): topEdge, ('faultSite', 'net', 'faultSite'): edge })\n",
    "    feats = torch.tensor([cir.Node[n].net.feats for n in cir.Node])\n",
    "    hg.nodes['faultSite'].data['feats'] = feats\n",
    "    hg.nodes['faultSite'].data['in_degree'] = hg.in_degrees(etype='net').view(-1,1).float()\n",
    "    hg.nodes['faultSite'].data['out_degree'] = hg.out_degrees(etype='net').view(-1,1).float()\n",
    "    hg.nodes['faultSite'].data['top_degree'] = hg.in_degrees(etype='topEdge').view(-1,1).float()\n",
    "    hg.nodes['faultSite'].data['level'] = getLevel(cir)\n",
    "    hg.nodes['faultSite'].data['loc'] = getLocation(cir, hg.num_nodes('faultSite'))\n",
    "    hg.nodes['faultSite'].data['more'] = addfeatures(cir, hg.num_nodes('faultSite'))\n",
    "    save_graphs(path, hg)\n",
    "    \n",
    "print(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intended-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1086147.,  862605.,  766332.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(hg.nodes['faultSite'].data['loc'], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innocent-amber",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-07T06:09:17.725251Z",
     "iopub.status.busy": "2021-04-07T06:09:17.723889Z",
     "iopub.status.idle": "2021-04-07T06:49:31.962584Z",
     "shell.execute_reply": "2021-04-07T06:49:31.961877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU time for preprocessing: 517.3026375770569\n"
     ]
    }
   ],
   "source": [
    "sample_path = design+\"/samples_600.bin\"\n",
    "\n",
    "if os.path.isfile(sample_path):\n",
    "    dataset, label_dict = load_graphs(sample_path)\n",
    "    labels = label_dict[\"labels\"]\n",
    "    subgraphs = []\n",
    "    for i in range(len(dataset)):\n",
    "        subgraphs.append((dataset[i], labels[i].item()))\n",
    "else:\n",
    "    start_pat = 0\n",
    "    end_pat = n_patterns\n",
    "\n",
    "    preprocess_st = time.time()\n",
    "\n",
    "    subgraphs, dstIDset = getDatasetfromLog(cir, design, dic, hg, n_patterns, 5000, start_pat, end_pat, True)\n",
    "    # subgraphs = getSubgraphs(hg, dataset, dstIDset, True, start_pat, end_pat)\n",
    "    print(\"Number of samples: {}\".format(len(subgraphs)))\n",
    "\n",
    "preprocess_end = time.time()\n",
    "\n",
    "print(\"Total CPU time for preprocessing: {}\".format(preprocess_end-preprocess_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = subgraphs[2][0]\n",
    "# cnt = 0\n",
    "# for n in cir.Node:\n",
    "#     if cir.Node[n].ID in sg.ndata[dgl.NID]['faultSite']:\n",
    "#         print(\"{}, {}, {}\".format(n,cir.Node[n].ID,cnt))\n",
    "#         cnt += 1\n",
    "g = dgl.to_homogeneous(sg)\n",
    "G = dgl.to_networkx(g)\n",
    "color_map = []\n",
    "for i in range(g.num_nodes()):\n",
    "    if hg.nodes['faultSite'].data['loc'][sg.ndata[dgl.NID]['faultSite'][i]][0] == 1:\n",
    "        color_map.append('blue')\n",
    "    elif  hg.nodes['faultSite'].data['loc'][sg.ndata[dgl.NID]['faultSite'][i]][1] == 1:\n",
    "        color_map.append('green')\n",
    "    elif  hg.nodes['faultSite'].data['loc'][sg.ndata[dgl.NID]['faultSite'][i]][2] == 1:\n",
    "        color_map.append('red')\n",
    "nx.draw(G, node_color=color_map, with_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "apart-statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 2024, 0: 1851, 1: 1125})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "labels = [i[1] for i in subgraphs]\n",
    "print(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "indoor-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GraphConv\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, 16)\n",
    "        self.linear1 = nn.Linear(16+3, num_classes)\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        \n",
    "#         print(h)\n",
    "#         h1 = F.softmax(h, dim=0)\n",
    "        ratio = torch.sum(g.ndata['infeats'][:,3:6], dim=0)/g.num_nodes()\n",
    "#         print(ratio)\n",
    "#         print(ratio.unsqueeze(0))\n",
    "#         loc = g.ndata['infeats'][:,3:6].argmax(0)\n",
    "#         print(loc)\n",
    "        \n",
    "        \n",
    "#         print(h)\n",
    "#         print(h*ratio)\n",
    "#         print(ratio)\n",
    "        \n",
    "        g.ndata['h_final'] = h\n",
    "        h_final = dgl.readout_nodes(g, 'h_final', op='max')\n",
    "#         return h_final\n",
    "        h_final = F.softmax(h_final, dim=0)\n",
    "        h_final = torch.cat([h_final, ratio.unsqueeze(0)], dim=1)\n",
    "#         print(h_final)\n",
    "#         print(dgl.readout_nodes(g, 'h2', op='max'))\n",
    "        return self.linear1(h_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suffering-worst",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-07T06:49:31.987591Z",
     "iopub.status.busy": "2021-04-07T06:49:31.985965Z",
     "iopub.status.idle": "2021-04-07T06:49:31.991188Z",
     "shell.execute_reply": "2021-04-07T06:49:31.991662Z"
    }
   },
   "outputs": [],
   "source": [
    "num_examples = len(subgraphs)\n",
    "num_train = int(num_examples * 0.7)\n",
    "num_val = int(num_examples * 0.15)\n",
    "\n",
    "random.shuffle(subgraphs)\n",
    "\n",
    "# # Random\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "val_sampler = SubsetRandomSampler(torch.arange(num_train, num_train+num_val))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train+num_val, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    subgraphs, sampler=train_sampler, batch_size=1, drop_last=False)\n",
    "val_dataloader = GraphDataLoader(\n",
    "    subgraphs, sampler=val_sampler, batch_size=1, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    subgraphs, sampler=test_sampler, batch_size=1, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-thompson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-07T06:49:32.024406Z",
     "iopub.status.busy": "2021-04-07T06:49:32.017338Z",
     "iopub.status.idle": "2021-04-07T07:08:07.154714Z",
     "shell.execute_reply": "2021-04-07T07:08:07.153232Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:\n",
      "Training...\n",
      "In epoch 0, train loss: 1.078, train acc: 0.387\n",
      "[[466. 292. 512.]\n",
      " [  0.   0.   0.]\n",
      " [836. 504. 890.]]\n",
      "Validation...\n",
      "In epoch 0, val loss: 1.072, val acc: 0.367\n",
      "[[273. 160. 315.]\n",
      " [  0.   2.   0.]\n",
      " [  0.   0.   0.]]\n",
      "\n",
      "Epoch 1:\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "train_begin = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = GCN(11,128,3)\n",
    "model = model.to('cuda')\n",
    "PATH = design+\"/saved_model\"\n",
    "# g = g.to('cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "train_acc_values = []\n",
    "val_acc_values = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    print(\"\\nEpoch %d:\" %epoch)\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    num_tests = 0\n",
    "    train_error = np.zeros((3,3))\n",
    "    for g, l in train_dataloader:\n",
    "        g = g.to('cuda')\n",
    "        labels = l.to('cuda')\n",
    "#         g = dgl.add_reverse_edges(g)\n",
    "#         g = dgl.add_self_loop(g)\n",
    "\n",
    "        infeats = g.ndata['infeats']\n",
    "#         g.ndata['w'] = g.ndata['infeats'][:,3:6]\n",
    "#         g.ndata['w'] = torch.matmul(g.ndata['infeats'][:,3:6], torch.sum(g.ndata['infeats'][:,3:6], dim=0)/g.num_nodes())\n",
    "#         print(g.ndata['w'].shape)\n",
    "\n",
    "\n",
    "        pred = model(g, infeats)\n",
    "#         print(pred.shape)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "#         l1_lambda = 0.001\n",
    "#         l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "#         loss = loss + l1_lambda * l1_norm\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += (pred.argmax(1) == labels).sum().item()\n",
    "        train_loss += loss\n",
    "        num_tests += len(labels)\n",
    "        for i in range(len(labels)):\n",
    "                p = pred[i]\n",
    "                l = labels[i]\n",
    "                train_error[p.argmax(0).item()][l.item()] += 1\n",
    "\n",
    "    avg_loss = train_loss/num_tests\n",
    "    avg_acc = train_acc/num_tests\n",
    "    train_loss_values.append(avg_loss)\n",
    "    train_acc_values.append(avg_acc)\n",
    "\n",
    "    print('In epoch {}, train loss: {:.3f}, train acc: {:.3f}'.format(epoch, avg_loss, avg_acc))\n",
    "    print(train_error)\n",
    "#     print(\"Mean correct: {}, Mean incorrect: {}\".format(np.mean(correct), np.mean(incorrect)))\n",
    "\n",
    "    print(\"Validation...\")\n",
    "    val_error = np.zeros((3,3))\n",
    "    model.eval()\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "    max_acc = 0\n",
    "    num_tests = 0\n",
    "\n",
    "#     random.shuffle(val_set)\n",
    "    with torch.no_grad():\n",
    "        for g, l in val_dataloader:\n",
    "            g = g.to('cuda')\n",
    "            labels = l.to('cuda')\n",
    "#             g = dgl.add_reverse_edges(g)\n",
    "#             g = dgl.add_self_loop(g)\n",
    "\n",
    "            infeats = g.ndata['infeats']\n",
    "#             g.ndata['w'] = g.ndata['infeats'][:,3:6]\n",
    "\n",
    "            pred = model(g, infeats)\n",
    "            loss = F.cross_entropy(pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            val_acc += (pred.argmax(1) == labels).sum().item()\n",
    "\n",
    "            val_loss += loss\n",
    "            num_tests += len(labels)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                p = pred[i]\n",
    "                l = labels[i]\n",
    "                val_error[p.argmax(0).item()][l.item()] += 1\n",
    "            \n",
    "    avg_loss = val_loss/num_tests\n",
    "    avg_acc = val_acc/num_tests\n",
    "    val_loss_values.append(avg_loss)\n",
    "    val_acc_values.append(avg_acc)\n",
    "\n",
    "    print('In epoch {}, val loss: {:.3f}, val acc: {:.3f}'.format(epoch, avg_loss, avg_acc))\n",
    "    print(val_error)\n",
    "    \n",
    "    if avg_acc > max_acc:\n",
    "        max_acc = avg_acc\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "#     \n",
    "train_end = time.time()\n",
    "print(\"\\nTraining time for {} epochs: {}\\n\".format(epoch+1, train_end-train_begin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-animation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-07T07:08:07.178104Z",
     "iopub.status.busy": "2021-04-07T07:08:07.176733Z",
     "iopub.status.idle": "2021-04-07T07:08:07.412159Z",
     "shell.execute_reply": "2021-04-07T07:08:07.413417Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(0,len(train_loss_values),1), train_loss_values,'b', np.arange(0,len(val_loss_values),1), val_loss_values, 'g')\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.ylim([0,1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(0,len(train_acc_values),1), train_acc_values,'b', np.arange(0,len(val_acc_values),1), val_acc_values, 'g')\n",
    "# plt.plot(np.arange(0,25,1), train_acc_values[-25:],'b', np.arange(0,25,1), val_acc_values[-25:], 'g')\n",
    "\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(11,128,3)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "test_acc = 0\n",
    "test_loss = 0\n",
    "num_tests = 0\n",
    "test_error = np.zeros((3,3))\n",
    "test_dist = np.zeros((3,3))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for g, l in test_dataloader:\n",
    "        g = g.to('cuda')\n",
    "        labels = l.to('cuda')\n",
    "        \n",
    "        infeats = g.ndata['infeats']\n",
    "\n",
    "        pred = model(g, infeats)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        test_acc += (pred.argmax(1) == labels).sum().item()\n",
    "        test_loss += loss\n",
    "        num_tests += len(labels)\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "                p = pred[i]\n",
    "                l = labels[i]\n",
    "                test_error[p.argmax(0).item()][l.item()] += 1\n",
    "                smax = F.softmax(p, dim=0)\n",
    "                test_dist[p.argmax(0).item()][l.item()] += abs(smax[p.argmax(0).item()].item() - smax[l.item()].item())\n",
    "        \n",
    "\n",
    "avg_loss = test_loss/num_tests\n",
    "avg_acc = test_acc/num_tests\n",
    "\n",
    "print(\"test accuracy: {}\".format(avg_acc))\n",
    "print(test_error)\n",
    "print(test_dist/test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "test_acc = 0\n",
    "test_loss = 0\n",
    "num_tests = 0\n",
    "test_error = np.zeros((3,3))\n",
    "test_dist = np.zeros((3,3))\n",
    "graphs = []\n",
    "g2 = []\n",
    "gl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for g, l in test_dataloader:\n",
    "        g = g.to('cuda')\n",
    "        labels = l.to('cuda')\n",
    "#         g = dgl.add_reverse_edges(g)\n",
    "#         g = dgl.add_self_loop(g)\n",
    "        infeats = g.ndata['infeats']\n",
    "        ubg = dgl.unbatch(g)\n",
    "\n",
    "        pred = model(g, infeats)\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        test_acc += (pred.argmax(1) == labels).sum().item()\n",
    "        test_loss += loss\n",
    "        num_tests += len(labels)\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            p = pred[i]\n",
    "            ll = labels[i]\n",
    "            \n",
    "            if p.argmax(0) != ll and p.argmax(0).item() == 2:\n",
    "                graphs.append(ubg[i].to('cpu'))\n",
    "                gl.append(ll.to('cpu'))\n",
    "            elif p.argmax(0) == ll and p.argmax(0).item() == 2:\n",
    "                g2.append(ubg[i].to('cpu'))\n",
    "\n",
    "avg_loss = test_loss/num_tests\n",
    "avg_acc = test_acc/num_tests\n",
    "\n",
    "print(\"test accuracy: {}\".format(avg_acc))\n",
    "print(test_error)\n",
    "print(test_dist/test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(graphs)):\n",
    "    g1 = graphs[j]\n",
    "    G = dgl.to_networkx(g1)\n",
    "    color_map = []\n",
    "    for i in range(g1.num_nodes()):\n",
    "        if g1.ndata['infeats'][i][3] == 1:\n",
    "            color_map.append('blue')\n",
    "        elif g1.ndata['infeats'][i][4] == 1:\n",
    "            color_map.append('green')\n",
    "        elif g1.ndata['infeats'][i][5] == 1:\n",
    "            color_map.append('red')\n",
    "    nx.draw(G, node_color=color_map, with_labels=True)\n",
    "    print(gl[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(g2)):\n",
    "    g1 = g2[j]\n",
    "    G = dgl.to_networkx(g1)\n",
    "    color_map = []\n",
    "    for i in range(g1.num_nodes()):\n",
    "        if g1.ndata['infeats'][i][3] == 1:\n",
    "            color_map.append('blue')\n",
    "        elif g1.ndata['infeats'][i][4] == 1:\n",
    "            color_map.append('green')\n",
    "        elif g1.ndata['infeats'][i][5] == 1:\n",
    "            color_map.append('red')\n",
    "    nx.draw(G, node_color=color_map, with_labels=True)\n",
    "#     print(gl[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [0]\n",
    "partitions.append(int(n_patterns/3))\n",
    "partitions.append(int(n_patterns/3)*2)\n",
    "partitions.append(int(n_patterns))\n",
    "print(\"Partitions: {}\".format(partitions))\n",
    "\n",
    "gg = getDatasetforVoting(cir, design, dic, hg, n_patterns, partitions, 8000, 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GCN()\n",
    "model1.load_state_dict(torch.load(\"tate_GNN/model_0_to_197\"))\n",
    "model1.eval()\n",
    "\n",
    "model2 = GCN()\n",
    "model2.load_state_dict(torch.load(\"tate_GNN/model_197_to_394\"))\n",
    "model2.eval()\n",
    "\n",
    "model3 = GCN()\n",
    "model3.load_state_dict(torch.load(\"tate_GNN/model_394_to_591\"))\n",
    "model3.eval()\n",
    "model1 = model1.to('cuda')\n",
    "model2 = model2.to('cuda')\n",
    "model3 = model3.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_tests = 0\n",
    "test_acc = 0\n",
    "for i in range(len(gg[0])):\n",
    "    results = np.zeros(3)\n",
    "    with torch.no_grad():\n",
    "        g, l = gg[0][i]\n",
    "        if g != 0:\n",
    "            g = g.to('cuda')\n",
    "            infeats = g.ndata['infeats']\n",
    "            pred = model1(g, infeats)\n",
    "            idx = pred.argmax(1).item()\n",
    "            results[idx] += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        g, l = gg[1][i]\n",
    "        if g != 0:\n",
    "            g = g.to('cuda')\n",
    "            infeats = g.ndata['infeats']\n",
    "            pred = model2(g, infeats)\n",
    "            idx = pred.argmax(1).item()\n",
    "            results[idx] += 1\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        g, l = gg[2][i]\n",
    "        if g != 0:\n",
    "            g = g.to('cuda')\n",
    "            infeats = g.ndata['infeats']\n",
    "            pred = model3(g, infeats)\n",
    "            idx = pred.argmax(1).item()\n",
    "            results[idx] += 1\n",
    "\n",
    "#     print(results)\n",
    "#     print(l)\n",
    "    \n",
    "    if np.sum(results) == 0:\n",
    "        continue\n",
    "    test_acc += (results.argmax(0) == l).sum().item()\n",
    "    num_tests += 1\n",
    "print(\"Test accuracy: {}\".format(test_acc/num_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-nelson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetforVoting(cir, design, dic, g, num_patterns, partitions, start_sample=1, end_sample=-1):\n",
    "    print(\"Start generating data for voting\")\n",
    "#     f1 = open(design+\"/\"+design+\"_inject_extra.dat\", \"r\")\n",
    "    f1 = open(design+\"/unique.dat\", \"r\")\n",
    "    l = f1.readlines()\n",
    "    f1.close()\n",
    "    l = l[start_sample:end_sample]\n",
    "    \n",
    "#     start_points = partitions[:-1]\n",
    "#     end_points = partitions[1:]\n",
    "    \n",
    "    subgraphs = [[] for i in range(len(partitions)-1)]\n",
    "\n",
    "    for line in l:\n",
    "        start_pat = partitions[0]\n",
    "        end_pat = partitions[-1]\n",
    "        \n",
    "        words = line.split()\n",
    "        gname = words[1].split(\"/\")[0]\n",
    "        pname = words[1].split(\"/\")[1]\n",
    "        logname = design+\"/Logs_w_MIV/\"+gname+\"_\"+pname+\"_st\"+words[0]+\".log\"\n",
    "\n",
    "        dstID = cir.Node[gname+\"_\"+pname].ID\n",
    "        label = -1\n",
    "        if g.nodes['faultSite'].data['loc'][dstID][0] == 1:\n",
    "            label = 0\n",
    "        elif g.nodes['faultSite'].data['loc'][dstID][1] == 1:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 2\n",
    "            \n",
    "        \n",
    "\n",
    "        f2 = open(logname, \"r\")\n",
    "        l2 = f2.readlines()[1:]\n",
    "        f2.close()\n",
    "        num_pat = end_pat-start_pat\n",
    "        r = np.zeros((g.number_of_nodes('topNode'), num_pat), dtype=np.dtype('float32'))\n",
    "        success = True\n",
    "        for fault in l2:\n",
    "            w2 = fault.split()\n",
    "            if len(w2) != 5:\n",
    "                success = False\n",
    "                break\n",
    "            pat = int(w2[0])-1\n",
    "\n",
    "#             if pat >= num_patterns:\n",
    "#                 break\n",
    "            if pat < start_pat:\n",
    "                continue\n",
    "\n",
    "            if pat >= end_pat:\n",
    "                break\n",
    "\n",
    "            chname = w2[1]\n",
    "            loc = int(w2[2])\n",
    "\n",
    "\n",
    "            chain = cir.scanchains[cir.sopin.index(chname)]\n",
    "            gname = chain[::-1][loc].name\n",
    "            srcID = dic[gname]\n",
    "            r[srcID][pat-start_pat] = 1.0\n",
    "\n",
    "        if not success:\n",
    "            continue\n",
    "            \n",
    "        getSubgraphsforVoting(g, r, label, dstID, partitions, subgraphs, True)\n",
    "            \n",
    "        if len(subgraphs[-1])%500 == 0:\n",
    "            print(len(subgraphs[-1]))\n",
    "    print(\"Finish generating data for voting\")\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  getSubgraphsforVoting(hg, d, l, dstID, partitions, subgraphs, debug=False):    \n",
    "    start_points = partitions[:-1]\n",
    "    end_points = partitions[1:]\n",
    "    \n",
    "    with hg.local_scope():\n",
    "        h = torch.from_numpy(d)\n",
    "        hg.nodes['topNode'].data['h'] = h\n",
    "        hg['topEdge'].update_all(message_func=fn.copy_u('h','m'), reduce_func=fn.sum('m', 'h'), etype='topEdge')\n",
    "        \n",
    "        for i in range(len(start_points)):\n",
    "            start_pat = start_points[i]\n",
    "            end_pat = end_points[i]\n",
    "            h_N = torch.mul(hg.nodes['faultSite'].data['h'][:,start_pat:end_pat], hg.nodes['faultSite'].data['feats'][:,start_pat:end_pat])\n",
    "            result = torch.sum(h[:,start_pat:end_pat],dim=0)\n",
    "            t = torch.all(h_N == result, dim=1).float()\n",
    "            nid = torch.nonzero(t, as_tuple=True)[0]\n",
    "            if len(nid) == 0:\n",
    "                subgraphs[i].append((0,0))\n",
    "                continue\n",
    "                \n",
    "            g = hg.subgraph({'faultSite': nid})\n",
    "\n",
    "            if debug:\n",
    "                assert(dstID in g.ndata[dgl.NID]['faultSite'])\n",
    "\n",
    "            infeats = torch.cat([g.nodes['faultSite'].data['in_degree'], g.nodes['faultSite'].data['out_degree'], g.nodes['faultSite'].data['top_degree']], dim=1)\n",
    "            infeats = torch.cat([infeats, g.nodes['faultSite'].data['loc']], dim=1)\n",
    "            infeats = torch.cat([infeats, g.nodes['faultSite'].data['level']], dim=1)\n",
    "            infeats = torch.cat([infeats, g.nodes['faultSite'].data['more']], dim=1)\n",
    "            infeats = torch.cat([infeats, g.in_degrees(etype='net').view(-1,1).float()], dim=1)\n",
    "            infeats = torch.cat([infeats, g.out_degrees(etype='net').view(-1,1).float()], dim=1)\n",
    "\n",
    "            g = dgl.to_homogeneous(g)\n",
    "            g.ndata['infeats'] = infeats\n",
    "            g = dgl.add_self_loop(g)\n",
    "            subgraphs[i].append((g,l))\n",
    "#     return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasetfromLog2(cir, design, dic, g, num_patterns, num_samples=-1, start_pat=0, end_pat=-1, shuffle=True):\n",
    "    print(\"Start generating data\")\n",
    "    \n",
    "    \n",
    "    dataset = []\n",
    "    dstIDset = []\n",
    "#     f1 = open(design+\"/\"+design+\"_inject_extra.dat\", \"r\")\n",
    "    if cir.design == \"ldpc_GNN\" or cir.design == \"tate_GNN\":\n",
    "        f1 = open(design+\"/unique.dat\", \"r\")\n",
    "    else:\n",
    "        f1 = open(design+\"/TDF_600_inject.dat\", \"r\")\n",
    "    l = f1.readlines()\n",
    "    f1.close()\n",
    "    l = l[1:]\n",
    "    if shuffle:\n",
    "        random.shuffle(l)\n",
    "\n",
    "    for line in l:\n",
    "        words = line.split()\n",
    "        gname = words[1].split(\"/\")[0]\n",
    "        pname = words[1].split(\"/\")[1]\n",
    "        if cir.design == \"ldpc_GNN\" or cir.design == \"tate_GNN\":\n",
    "            logname = design+\"/Logs_w_MIV/\"+gname+\"_\"+pname+\"_st\"+words[0]+\".log\"\n",
    "        else:\n",
    "            logname = design+\"/Logs_w_MIV_TDF_600/\"+gname+\"_\"+pname+\"_st\"+words[0]+\".log\"\n",
    "        \n",
    "        if words[1].split(\"/\")[-1] == \"nextstate\":\n",
    "            pname = \"D\"\n",
    "        elif words[1].split(\"/\")[-1] == \"IQ\":\n",
    "            pname = \"Q\"\n",
    "        \n",
    "        if gname.startswith(\"MIV\") and pname == \"A\":\n",
    "            pname = \"Q\"\n",
    "\n",
    "        dstID = cir.Node[gname+\"_\"+pname].ID\n",
    "        label = -1\n",
    "        if g.nodes['faultSite'].data['loc'][dstID][0] == 1:\n",
    "            label = 0\n",
    "        elif g.nodes['faultSite'].data['loc'][dstID][1] == 1:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 2\n",
    "            \n",
    "        if not os.path.isfile(logname):\n",
    "            continue\n",
    "            \n",
    "        print(logname)\n",
    "        f2 = open(logname, \"r\")\n",
    "        l2 = f2.readlines()[1:]\n",
    "        f2.close()\n",
    "\n",
    "        \n",
    "        num_pat = end_pat-start_pat\n",
    "        cur_pat = -1\n",
    "        cnt_per_pat = 0\n",
    "        success = True\n",
    "        subnodes = []\n",
    "        srcIDlist = []\n",
    "        patlist = []\n",
    "        \n",
    "        for fault in l2:\n",
    "            w2 = fault.split()\n",
    "            if len(w2) != 5:\n",
    "#                 continue\n",
    "                success = False\n",
    "                break\n",
    "            pat = int(w2[0])-1\n",
    "\n",
    "            if pat < start_pat:\n",
    "                continue\n",
    "            \n",
    "            if pat >= end_pat:\n",
    "                break\n",
    "            \n",
    "            chname = w2[1]\n",
    "            loc = int(w2[2])\n",
    "\n",
    "\n",
    "            chain = cir.scanchains[cir.sopin.index(chname)]\n",
    "            gname = chain[::-1][loc].name\n",
    "            srcID = dic[gname]\n",
    "            srcIDlist.append(srcID)\n",
    "            patlist.append(pat-start_pat)\n",
    "            \n",
    "            tmpnodes =  g.successors(srcID, etype=('topNode', 'topEdge', 'faultSite')).numpy()\n",
    "        \n",
    "            if len(subnodes):\n",
    "                tmpnodes = np.intersect1d(subnodes, tmpnodes)\n",
    "            \n",
    "     \n",
    "            subnodes = np.array([idx for idx in tmpnodes if g.nodes['faultSite'].data['feats'][idx][pat-start_pat] == 1.0])\n",
    "\n",
    "            if not len(subnodes):\n",
    "                break\n",
    "\n",
    "#             if cur_pat == -1 or cur_pat == pat:\n",
    "#                 cnt_per_pat += 1\n",
    "#                 cur_pat = pat\n",
    "#                 continue\n",
    "#             elif pat != cur_pat:\n",
    "#                 tmp = []\n",
    "#                 for idx in subnodes:\n",
    "#                     valid = True\n",
    "#                     cnt = 0\n",
    "#                     for topidx in g.predecessors(idx, etype=('topNode', 'topEdge', 'faultSite')):\n",
    "#                         if g.nodes['topNode'].data['feats'][topidx][cur_pat-start_pat] == 1.0:\n",
    "#                             cnt += 1\n",
    "#                     if cnt == cnt_per_pat:\n",
    "#                         tmp.append(idx)\n",
    "                \n",
    "#                 subnodes = tmp\n",
    "#                 print(tmp)\n",
    "#                 if not len(tmp):\n",
    "#                     break\n",
    "                    \n",
    "#                 cur_pat = pat\n",
    "#                 cnt_per_pat = 0\n",
    "            \n",
    "        if not success:\n",
    "            continue\n",
    "        if not len(subnodes):\n",
    "            print(\"No candidates!!!, {}\".format(logname))\n",
    "            continue\n",
    "        \n",
    "        print(subnodes)\n",
    "        tot = len(patlist)\n",
    "        patlist = np.unique(patlist)\n",
    "        tmp = []\n",
    "        for idx in subnodes:\n",
    "            cnt = 0\n",
    "            for topidx in g.predecessors(idx, etype=('topNode', 'topEdge', 'faultSite')):\n",
    "                for p in patlist:\n",
    "                    if g.nodes['topNode'].data['feats'][topidx][p] == 1.0:\n",
    "                        cnt += 1\n",
    "            tmp.append(cnt-tot)\n",
    "        print(tmp)\n",
    "#         print(np.unique(srcIDlist))\n",
    "        with g.local_scope():\n",
    "            sg = g.subgraph({'faultSite': subnodes})\n",
    "\n",
    "            print(\"dstID: {}\".format(dstID))\n",
    "            assert(dstID in sg.ndata[dgl.NID]['faultSite'])\n",
    "#             print(sg.ndata[dgl.NID]['faultSite'])\n",
    "\n",
    "            infeats = torch.cat([sg.nodes['faultSite'].data['in_degree'], sg.nodes['faultSite'].data['out_degree'], sg.nodes['faultSite'].data['top_degree']], dim=1)\n",
    "            infeats = torch.cat([infeats, sg.nodes['faultSite'].data['loc']], dim=1)\n",
    "            infeats = torch.cat([infeats, sg.nodes['faultSite'].data['level']], dim=1)\n",
    "#             infeats = torch.cat([infeats, sg.nodes['faultSite'].data['more']], dim=1)\n",
    "#             infeats = torch.cat([infeats, sg.in_degrees(etype='net').view(-1,1).float()], dim=1)\n",
    "#             infeats = torch.cat([infeats, sg.out_degrees(etype='net').view(-1,1).float()], dim=1)\n",
    "\n",
    "            sg = dgl.to_homogeneous(sg)\n",
    "            sg = dgl.add_reverse_edges(sg)\n",
    "            sg.ndata['infeats'] = infeats\n",
    "            \n",
    "        sg = dgl.add_self_loop(sg)\n",
    "        \n",
    "        dataset.append((sg, label))\n",
    "        dstIDset.append(dstID)\n",
    "        \n",
    "        if len(dataset)%5 == 0:\n",
    "            print(len(dataset))\n",
    "        \n",
    "        if len(dataset) == num_samples:\n",
    "            break\n",
    "            \n",
    "    print(\"Finish generating data\")\n",
    "    return dataset, dstIDset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
